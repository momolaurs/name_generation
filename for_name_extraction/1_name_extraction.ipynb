{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_lg\n",
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x2YMsc8r5kr",
        "outputId": "1c2fdf3c-e222-411d-c87d-3ce430d00219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.7.0/es_core_news_lg-3.7.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-lg==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-lg==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "6KhrUOpJtAEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_names_ner(input_file, output_file, chunk_size=1000000):\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts Spanish person names from a corpus file using spaCy.\n",
        "\n",
        "    Parameters:\n",
        "    - input_file (str): Path to the input corpus file.\n",
        "    - output_file (str): Path to the output file to save the extracted names.\n",
        "    - chunk_size (int): Size of chunks to process the input file (spaCy limit is set to 1,000,000).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the Spanish language model and disable unnecessary components\n",
        "    nlp = spacy.load(\"es_core_news_lg\")\n",
        "    all_names = []\n",
        "\n",
        "    # Read the content from the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "\n",
        "        print(\"Extracting names...\\n\")\n",
        "        #Progress bar\n",
        "        total_size = os.path.getsize(input_file)  #Obtain total size\n",
        "        progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Processing\", position = 0) #Initialize\n",
        "\n",
        "        #Read the file in chunks\n",
        "        chunk = file.read(chunk_size)\n",
        "        while chunk:\n",
        "            doc = nlp(chunk) #Process chunk with spaCy\n",
        "            names = [ent.text for ent in doc.ents if ent.label_ == \"PER\"] # Extract person names using spaCy NER\n",
        "\n",
        "            # Add names from the current chunk to the list\n",
        "            all_names.extend(names)\n",
        "\n",
        "            # Update the progress bar\n",
        "            progress_bar.update(len(chunk))\n",
        "\n",
        "            # Read the next chunk\n",
        "            chunk = file.read(chunk_size)\n",
        "\n",
        "    # Close the progress bar\n",
        "    progress_bar.close()\n",
        "    print(\"\\nDone!\")\n",
        "\n",
        "    # Print a sample of extracted names for verifying correct extraction\n",
        "    print(\"\\nSample of extracted names:\")\n",
        "    for name in all_names[-5:]:\n",
        "        print(name)\n",
        "\n",
        "    # Save names to the output file\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        for name in all_names:\n",
        "            output_file.write(name + '\\n') #add newlines between each extracted name\n",
        "\n",
        "extract_names_ner('old_corpus.txt', 'temp_old_names.txt')"
      ],
      "metadata": {
        "id": "zAoKkgnns_Ml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2b16f0-52e0-4ea0-ee2b-00eb6e2bf817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting names...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Filtering:   0%|          | 0/646604 [54:11<?, ?line/s]\n",
            "Processing:  97%|█████████▋| 18.6M/19.1M [11:40<00:18, 26.5kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done!\n",
            "\n",
            "Sample of extracted names:\n",
            "Nina\n",
            "Nina\n",
            "señá Benina\n",
            "Nina\n",
            "Nina\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_names = []\n",
        "\n",
        "#Make second pass with the txt file generated in the previous step\n",
        "\n",
        "def ner_for_persons(input_file, output_file):\n",
        "    print(\"Second pass...\\n\")\n",
        "    # Load the Spanish language model\n",
        "    nlp = spacy.load(\"es_core_news_lg\", disable=[\"tagger\", \"parser\"])  # Disable unnecessary components for speed\n",
        "\n",
        "    # Read the content from the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Process with the spaCy pipeline\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract persons (PER) using NER\n",
        "    persons = [ent.text for ent in doc.ents if ent.label_ == \"PER\"]\n",
        "    all_names.extend(persons)\n",
        "\n",
        "    # Save the names to the output file\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        for name in all_names:\n",
        "            output_file.write(name + '\\n')\n",
        "\n",
        "ner_for_persons('temp_old_names.txt', 'temp_old_names.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SunGENe1foXB",
        "outputId": "4a8089f7-813c-4206-f1fb-aba0771774e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second pass...\n",
            "\n",
            "Extracted Persons:\n",
            "Francisca\n",
            "Trasladose\n",
            "Frasquito Ponte Delgado\n",
            "Juliana\n",
            "Juliana\n",
            "Juliana\n",
            "Antonio\n",
            "Almudena\n",
            "Juliana\n",
            "Benina\n",
            "Romualdo\n",
            "Doña Paca\n",
            "Nina\n",
            "Juliana\n",
            "Sabe\n",
            "Paquito\n",
            "Antoñito\n",
            "Grande\n",
            "Juliana\n",
            "Nina\n",
            "Nina\n",
            "Benina\n",
            "Nina\n",
            "Nina\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_names(input_file, output_file):\n",
        "    print(\"Creating a cleaner set of names...\\n\")\n",
        "\n",
        "    # Read the list of names from the file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        names = [line.strip() for line in file if line.strip()]\n",
        "    # Remove duplicates\n",
        "    unique_names = list(set(names))\n",
        "\n",
        "    # Write the names back to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as output_file:\n",
        "        for name in unique_names:\n",
        "            output_file.write(name + '\\n')\n",
        "\n",
        "    return print(f\"Extracted names have been saved in your directory as '{output_file.name}'\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "clean_names('temp_old_names.txt', 'temp_old_names.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMj7cc5Fc5md",
        "outputId": "dfec7a2c-2843-4e79-e31b-2e507198110b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a cleaner set of names...\n",
            "\n",
            "Extracted names have been saved in your directory as 'temp_old_names.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_names(sheet_name):\n",
        "    \"\"\"\n",
        "    Reads names from an Excel file\n",
        "\n",
        "    Parameters:\n",
        "    - sheet_name (str): The sheet name to read from the Excel file.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of names in lowercase.\n",
        "    \"\"\"\n",
        "    #Read in Excel file\n",
        "    df = pd.read_excel('nombres_por_edad_media.xls', sheet_name=sheet_name, skiprows=6)\n",
        "\n",
        "    #Convert to lowercase and return as list\n",
        "    return [str(name).lower() for name in df['Nombre'].tolist()]\n",
        "\n",
        "names_modern = read_names('Hombres') + read_names('Mujeres')"
      ],
      "metadata": {
        "id": "xN2h_fahDR6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_names(input_file, output_file, filter_list):\n",
        "    \"\"\"\n",
        "    Filters lines from an input file based on a list of names.\n",
        "\n",
        "    Parameters:\n",
        "    - input_file (str): Path to the input text file to filter.\n",
        "    - output_file (str): Path to the output file to save the filtered lines.\n",
        "    - filter (list): List of names/words to use for filtering.\n",
        "\n",
        "    \"\"\"\n",
        "    # Read the lines from the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = [line.strip() for line in file]\n",
        "\n",
        "    total_lines = len(lines)\n",
        "\n",
        "    print(\"Filtering dictionary words to remove names...\\n\")\n",
        "\n",
        "    # Initialize the progress bar\n",
        "    progress_bar = tqdm(total=total_lines, unit='line', desc=\"Filtering\", position = 0)\n",
        "\n",
        "    # Exclude lines that match the names from filter file (names_modern)\n",
        "    filtered_lines = []\n",
        "    for idx, line in enumerate(lines):\n",
        "        if line.lower() not in filter_list: #check whether the line is not present in input file\n",
        "            filtered_lines.append(line)\n",
        "\n",
        "        # Update the progress bar every 10%\n",
        "        if (idx + 1) % (total_lines // 10) == 0:\n",
        "          progress_bar.update(total_lines // 10)\n",
        "\n",
        "    # Close the progress bar\n",
        "    progress_bar.close()\n",
        "    print(\"\\nDone!\")\n",
        "\n",
        "    # Write the filtered lines back to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as output_file:\n",
        "        for line in filtered_lines:\n",
        "            output_file.write(line + '\\n')\n",
        "\n",
        "    #Return path to output file\n",
        "    return print(f\"Filtered output has been saved in your directory as '{output_file.name}'\")\n",
        "\n",
        "filter_names('palabras_DLE.txt', 'temp_filtered_dictionary.txt', names_modern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYI6ZVP-38jx",
        "outputId": "bb0d45a6-3e6c-4cc0-e12d-0ed30552baf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering dictionary words to remove names...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Filtering: 100%|█████████▉| 646600/646604 [11:50<00:00, 909.69line/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done!\n",
            "Filtered output has been saved in your directory as 'temp_filtered_dictionary.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_through_dictionary(input_file, filter, output_file):\n",
        "\n",
        "    \"\"\"\n",
        "    Filters lines from an input file based on a dictionary or list of filter words.\n",
        "\n",
        "    Parameters:\n",
        "    - input_file (str): Path to the input text file to filter.\n",
        "    - filter (str): Path to the file containing filter words.\n",
        "    - output_file (str): Path to the output file to save the filtered lines.\n",
        "\n",
        "    \"\"\"\n",
        "    # Read the lines from input file and filter\n",
        "    with open(input_file, 'r', encoding='utf-8') as input, open(filter, 'r', encoding='utf-8') as filter:\n",
        "        lines_input = [line.strip().lower() for line in input]\n",
        "        lines_filter = [line.strip().lower() for line in filter]\n",
        "\n",
        "    print(\"Filtering names through filtered dictionary words...\\n\")\n",
        "    # Remove lines from input that are also in filter and create final set\n",
        "    filtered_lines = [line for line in lines_input if line not in lines_filter]\n",
        "\n",
        "    # Write the filtered lines back to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as output:\n",
        "        for line in filtered_lines:\n",
        "            output.write(line + '\\n')\n",
        "\n",
        "filter_through_dictionary('temp_old_names.txt', 'temp_filtered_dictionary.txt', 'temp_0_old_names_final.txt')"
      ],
      "metadata": {
        "id": "BLxWJi76Loa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9455455e-f781-46ea-f0dd-484407035812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering names through filtered dictionary words...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def old_names_final(input_file, output_file):\n",
        "    # Read the list of names from the file\n",
        "    print(\"Retrieving unique names...\")\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        names = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_names = list(set(names))\n",
        "\n",
        "    # Write the cleaned names back to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as output_file:\n",
        "        for name in unique_names:\n",
        "            output_file.write(name + '\\n')\n",
        "\n",
        "    print('Almost done...')\n",
        "\n",
        "    # Cleanup - Remove all files that begin with \"temp\"\n",
        "    temp_files = []\n",
        "    for temp_file in os.listdir():\n",
        "        if temp_file.startswith(\"temp_\"):\n",
        "            temp_files.append(temp_file)\n",
        "            os.remove(temp_file)\n",
        "\n",
        "    if temp_files:\n",
        "        print(\"Removed temporary files.\")\n",
        "    else:\n",
        "        print(\"No temporary files found.\")\n",
        "\n",
        "    print(f\"Final name list has been saved in your directory as '{output_file.name}'! Ready for training. :)\")"
      ],
      "metadata": {
        "id": "AgLDRW7Dl6tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_names_final('temp_0_old_names_final.txt', '0_old_names_final.txt')"
      ],
      "metadata": {
        "id": "Zn3JZTmI3Pch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}